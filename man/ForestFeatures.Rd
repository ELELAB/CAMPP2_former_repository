% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Forest_Features.R
\name{ForestFeatures}
\alias{ForestFeatures}
\title{Random forest fitting and variable selection}
\usage{
ForestFeatures(
  seed,
  data,
  group,
  validation = FALSE,
  num.trees.init,
  num.trees.iterat
)
}
\arguments{
\item{seed}{an integer containing a random seed number.}

\item{data}{a matrix of (transformed and normalized) feature counts from
"seq", "array", "ms" or "other" technology (with feature IDs as row names
and sample IDs as columns)}

\item{group}{a factor specifying group for each sample (e.g. could be
represented by a column from a metadata file)}

\item{validation}{a boolean indicating if validation will be performed
on test data. If TRUE validation will be performed on test data. If FALSE
validation will not be performed on test data. Default is FALSE.}

\item{num.trees.init}{an integer specifying number of trees to use for the first
forest in the feature selection process}

\item{num.trees.iterat}{an integer specifying number of trees to use for
all additional forests in the feature selection process}
}
\value{
a list of four elements: 1) a varSelRF object containing results of variable
selection using random forest, 2) a randomForest object containing random forest model
fitted to data, 3) a factor containing predictions of test data using fitted random
forest model and 4) a confusionMatrix object containing confusion matrix of test data
using fitted random forest model.
If validation = FALSE, the last three elements in output will be NA.
}
\description{
This function implements random forest on feature counts
(e.g. genes) and allows for feature selection using the random forest
algorithm. For the feature selection process, the recommended value for
number of trees is 5000 for the first forest and 2000 for all additional
forests.
In the variable selection process, variables are eliminated iteratively
by excluding the least important variables from each random forest. 20% of
the variables are excluded following each iteration. The out-of-bag (OOB)
error is used as criterion for determining the final selected variables.
Besides variable selection, a random forest model is also fitted which
is used for classification of the samples.
The input data is split into training and validation datasets where the
training data is used for variable selection and classification and the
random forest classifier is validated on the validation dataset.
}
\examples{
\dontrun{
campp2_brca_1_forest_features <-
ForestFeatures(seed = 173,
data = campp2_brca_1_batchCorrected,
group = campp2_brca_1_meta$diagnosis,
validation = TRUE,
num.trees.init = 30,
num.trees.iterat = 15)
}
}
