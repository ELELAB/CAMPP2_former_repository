#' @title CAMPP2 pipeline
#' @param data1 Gene count matrix (gene IDs as row names and sample IDs as columns). It's recommended to import gene counts using function "import_counts".
#' @param data2 Gene count matrix for a second dataset.
#' @param metadata1 Samples' metadata table should be imported using function "import_metadata". Metadata must include exactly the same samples as gene counts (data1) and samples must be sorted similarly.
#' @param metadata2 Metadata for a second dataset.
#' @param technology Technology used for the analysis of biological input. Current options are 'array', 'seq', 'ms' or 'other'. This argument is mandatory and depending on which option is chosen, data is transformed differently. If a second dataset is provided, the option should be specified for each dataset, provided as a character vector.
#' @param groups Argument defining groups of samples should be specified as a character vector. The first element specifying the name of the column in the metadata file containing sample IDs and the second element specifying the name of the column which contains the groups for the DEA analysis.
#' @param control.group A string vector defining control group name (e.g., "healthy" or "normal"). In case of subtype analysis (>2 groups), the output of the main wrapper will include comparisons between control group and each subtype.
#' @param data.check Distributional checks of the input data is activated using logical argument (TRUE/FALSE). If activated, Cullen-Frey graphs will be made for 10 randomly selected variables to check data distributions. This argument is per default set to TRUE.
#' @param kmeans Argument specifies ("TRUE" or "FALSE") if a k-means clustering should be performed. Default is FALSE (do not run).
#' @param num.km.clusters either a vector of manually defined number(s) of clusters, or NULL. Each number in this vector represent a plausible number of clusters that the user would expected to be present in the data. If multiple values provided, the function will automatically perform K-means clustering using each of them as k (expected number of clusters) separately. If this argument is NULL (default), optimal numbers of clusters are calculated automatically based on the bayesian information criterion (mclust package), applied to sub sampled data (see documentation for the whole procedure).
#' @param batches Specifies which metadata should be used for a batch correction (sequencing run/tissue/interstitial fluid/etc.). Argument takes a character vector of length 1 (one data set) or 2 (two data sets), where the string(s) match a column name(s) in the metadata file(s). In case batch correction should be performed only in 1 out of 2 data sets, a data set without the batch correction (1st one in the example) should be define as "", e.g. batches(c("","column_name")). Default is NULL.
#' @param plot.heatmap Argument defining which data will be used for the selection of the top x features to be plotted on the heatmap. Options are:"DEA", "LASSO", "EN", "Ridge" or "Consensus".
#' @param heatmap.size Argument specifying how many genes will be selected to be plotted on the heatmap if plot.heatmap is TRUE. The input must be specified as an even number. Default is 30.
#' @param show.PCA.labels a boolean value (TRUE or FALSE) specifying if elements (e.g. samples) should be labelled (for PCAPlot and runKmeans functions). Labeling is based on column names of the input data. Default value is FALSE.
#' @param viridis.palette Argument specifying viridis color palette used for heatmaps. Default is "turbo".
#' @param show.PCA.labels a boolean value (TRUE or FALSE) specifying if elements (e.g. samples) should be labelled (for PCAPlot and runKmeans functions). Labeling is based on column names of the input data. Default value is FALSE.
#' @param plot.PCA Argument specifies ("TRUE" or "FALSE") if a PCA plot (describing relationships between the samples based on feature counts and sample groups; generated by PCAPlot function) should be created for data overview. Default is FALSE (do not run).
#' @param standardize (double check for sequencing) Data centering. This option may be set to "mean" or "median." If two datasets are provided, the standardize option should be specified for each dataset, provided as a character vector. If the argument standardize is not specified and "technology" = "array", then quantile normalization will be performed. Defaults is FALSE (do not run).
#' @param transform Data transformation type. Current options are "log2", "log10", "logit" and "voom". If two datasets are provided the parameter should be specified for each dataset, provided as a character vector. Defaults is FALSE (do not run).
#' @param prefix Prefix for the results' files and results folder. Default is "Results".
#' @param plot.DEA This argument specifies ("TRUE" or "FALSE") whether visualizations should be made for the differential expression analysis. Visualizations include a Volcano plot for the groups of samples and an Upset plot and Venn diagram for sample subtypes in the input data if subtypes are present.
#' @param signif Cut-offs for log fold change (logFC) and corrected p-value (fdr), defining significant hits (proteins, genes, miRNAs or N-features). If argument is set, it must be a numeric vector, where the first element specifies the cut-off for logFC and the second element specifies the cut-off for corrected p-value (fdr).  In case of 2 datasets, vector must be of length 4. By default, cutoffs will be set to -1 > logFC > 1 and corrected p-values < 0.05.
#' @param ensembl.version This argument specifies which ENSEMBL database to use when transforming ensemble IDs into HUGO IDs using biomaRt. The argument should be specified as a number.
#' @param covariates Covariates are specified as a character vector. Specified covariates will be included in both DEA analysis. Names of covariates should match the desired columns in the metadata file. Only one covariate for each dataset is allowed (multiple covariates are allowed when using RunDEA function out of this wrapper). Default is NULL.
#' @param colors Custom color palette for PCA and heatmaps. Must be the same length as number of groups used for comparison (e.g. two groups = two colors) and must be defined as character vector. See R site for avalibe colors http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf. Default is NULL.
#' @param block A vector or factor specifying a blocking variable for differential expression analysis. The block must be of same length as the belonging dataset and contain 2 or more options. For 2 datasets the block can be defined as a list of factors or vectors.
#' @param alpha.lasso a numeric vector specifying hyperparameter alpha for LASSO/Elastic network/Ridge regression. This value must be set to 0.0 < x < 1.0 for Elastic Net or to 1.0 for LASSO regression or to 0.0 for Ridge regression. Defaults is FALSE (do not run).
#' @param min.coef.lasso a numeric vector specifying a threshold for features' filtering (e.g. genes) based on the coefficients which are calculated during model fitting. Default value is > 0.
#' @param nfolds.lasso a numeric vector describing number of folds during Lambda estimation which is based on a cross-validation. Although nfolds can be as large as the sample size (leave-one-out CV), it is not recommended for large datasets. Smallest value allowable is nfolds=3. Default is 10.
#' @param num.trees.init an integer specifying number of trees to use for the first random forest in the random forest feature selection process. Default is NULL (not activated). Both num.trees.init and num.trees.iterat need to be > 0 to activate random forest.
#' @param num.trees.iterat an integer specifying number of trees to use for all additional random forests in the random forest feature selection process. Default is NULL (not activated). Both num.trees.init and num.trees.iterat need to be > 0 to activate random forest.
#' @param split.size an integer specifying the minimum number of samples that the groups must contain in order to carry out random forest classification and subsequent validation
#' @param test.train.ratio a floating point number between 0 and 1 representing the ratio of samples to keep as validation dataset. For example, a test.train.ratio = 0.25 splits 25 percent of the data into a validation dataset, meaning 75 percent of the data will be kept as the training dataset.
#' @import zeallot
#' @import rio
#' @export
#' @return CAMPP2 results
#' @examples {
#' runCampp2(batches=c("tumor_stage","tumor_stage"),prefix="test_CAMPP2_distr", data1=campp2_brca_1, data2=campp2_brca_2, metadata1=campp2_brca_1_meta,metadata2=campp2_brca_2_meta, groups=c("IDs", "diagnosis","IDs", "diagnosis"), technology=c("seq","seq"), plot.PCA=TRUE, plot.DEA=TRUE, control.group = c("healthy","healthy"), plot.heatmap="DEA", alpha.lasso=0.5, data.check=TRUE, num.trees.init=5000, num.trees.iterat=2000, split.size=5, test.train.ratio=0.25, covariates = c("tumor_stage", "tumor_stage"))
#' runCampp2(batches=c("tumor_stage"),prefix="test_CAMPP2_distr", data1=campp2_brca_1, metadata1=campp2_brca_1_meta,groups=c("IDs", "diagnosis"), technology=c("seq"), data.check=TRUE, num.trees.init=5000, num.trees.iterat=2000, split.size=5, test.train.ratio=0.25)
#' }

runCampp2 <- function (data1, metadata1, data2=NULL, metadata2=NULL, technology, groups, control.group=NULL, batches=NULL, data.check=TRUE, standardize=FALSE, transform=FALSE, plot.PCA=FALSE, plot.heatmap=FALSE, kmeans=FALSE, ensembl.version=104, plot.DEA=FALSE, heatmap.size=40, viridis.palette="turbo", num.km.clusters=NULL, signif=NULL, block=NULL, colors=NULL, prefix="Results", covariates=NULL, show.PCA.labels=FALSE, alpha.lasso=FALSE, min.coef.lasso=NULL, nfolds.lasso=NULL, num.trees.init=NULL, num.trees.iterat=NULL, split.size=NULL, test.train.ratio=NULL){

    ###parse input arguments and assign updated values
    c(data1,data2,metadata1,metadata2,technology,groups,
      group1,group2,control.group1,control.group2,ids,batches,databatch1,databatch2,
      batch1,batch2,standardize,transform,data.check,
      plot.PCA,kmeans,num.km.clusters,cutoff.logFC1,cutoff.FDR1,
      cutoff.logFC2,cutoff.FDR2,block,block1,block2,colors,prefix,plot.heatmap,
      covarDEA1,covarDEA2,
      DEA.allowed.type,
      show.PCA.labels,heatmap.size,viridis.palette,ensembl.version,plot.DEA,
      alpha.lasso, min.coef.lasso, nfolds.lasso, num.trees.init, num.trees.iterat, split.size, test.train.ratio) %<-% parseArguments(data1=data1, metadata1=metadata1, data2=data2, metadata2=metadata2,
                                                technology=technology, groups=groups, control.group, batches=batches,
                                                data.check=data.check, standardize=standardize, transform=transform,
                                                plot.PCA=plot.PCA, plot.heatmap=plot.heatmap, kmeans=kmeans,
                                                signif=signif, block=block, colors=colors, prefix=prefix,
                                                covariates=covariates, show.PCA.labels=show.PCA.labels,
                                                num.km.clusters=num.km.clusters, plot.DEA=plot.DEA, heatmap.size=heatmap.size,viridis.palette=viridis.palette,ensembl.version=ensembl.version,
                                                alpha.lasso=alpha.lasso, min.coef.lasso=min.coef.lasso,
                                                nfolds.lasso=nfolds.lasso, num.trees.init=num.trees.init, num.trees.iterat=num.trees.iterat,
                                                split.size=split.size, test.train.ratio=test.train.ratio)



    dir.create(prefix)
    setwd(paste0(prefix, "/"))


    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    #                                                                         ## MISSING VALUE IMPUTATIONS ###
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

    print("RUNNING MISSING VALUE IMPUTATIONS")
    print("Running missing values imputation on data1")

    data1<-ReplaceNAs(data1)

    print("Missing values imputation on data1 has finished")

    if (!is.null(data2)){
        print("Running missing values imputation on data2")

        data2<-ReplaceNAs(data2)

        print("Missing values imputation on data2 has finished")
    }

    ###saving the results
    dir.create("ReplaceNAs")
    setwd("ReplaceNAs/")
    save(data1,file="data1_ReplaceNAs.rda")
    if(!is.null(data2)){
        save(data2,file="data2_ReplaceNAs.rda")
    }
    setwd("../")

    print("MISSING VALUE IMPUTATIONS FINISHED")

    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    #                                                                         ## Fix zeros and check for negative values. ###
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

    print("CAMPP2 automatically detects negative values and fix zeros in your data")
    print("RUNNING DETECTION OF NEGATIVE VALUES AND FIXING OF ZERO VALUES")
    print("Detecting negative values and fixing zeros in data1")

    data1.original <- data1
    data1 %<-% FixZeros(data1,group1)

    data2.original=NULL
    if (!is.null(data2)){
        data2.original <- data2
        print("Detecting negative values and replacing zeros in data2")
        data2 %<-% FixZeros(data2,group2)
    }

    ###saving the results
    dir.create("FixZeros")
    setwd("FixZeros/")
    save(data1,file="data1_FixZeros.rda")
    if(!is.null(data2)){
        save(data2,file="data2_FixZeros.rda")
    }
    setwd("../")

    print("FIXING OF NEGATIVE AND ZERO VALUES FINISHED")

    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    #                                                                         ## Normalization, Filtering and Transformation ###
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

    print("CAMPP2 automatically performs data normalization and transformation depending on technology from which data are derived.")
    print("PROCESSING NORMALIZATION AND TRANSFORMATION")
    print("Normalizing and tranforming data1")

    if(!is.null(data1.original) && technology[1] == "seq"){    ###Only sequencing data could be normalized with 0-values
        print("Original data1 including 0-values are being normalized")
        data1 <- NormalizeData(data1.original, group1, transform[1], standardize[1], technology[1])
    } else {
        print("data1 without 0-values are being normalized")
        data1 <- NormalizeData(data1, group1, transform[1], standardize[1], technology[1])
    }

    if(!is.null(data2) || !is.null(data2.original)){
        print("Normalizing and tranforming data2")
        if(!is.null(data2.original) && technology[2] == "seq"){       ###Only sequencing data could be normalized with 0-values
            print("Original data2 including 0-values are being normalized")
            data2 <- NormalizeData(data2.original, group2, transform[2], standardize[2], technology[2])
        } else {
            print("data2 without 0-values are being normalized")
            data2 <- NormalizeData(data2, group2, transform[2], standardize[2], technology[2])
            }
    }

    ###saving the results
    dir.create("NormalizeTransform")
    setwd("NormalizeTransform/")
    save(data1,file="data1_NormTrans.rda")
    if(!is.null(data2)){
        save(data2,file="data2_NormTrans.rda")
    }
    setwd("../")

    print("PROCESSING NORMALIZATION AND TRANSFORMATION FINISHED")


    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    ### BATCH CORRECTION ###
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

    print("RUNNING BATCH CORRECTION")

    ###This should consider scenario when only 2nd data set should be batch corrected
    if (databatch1==TRUE){
        print("Run batch correction on the 1st dataset")
        data1.batch %<-% BatchCorrect(data1,batch1,group1,technology[1])
        print("Batch correction of the first dataset finished")
    }else if (!is.null(data1) && databatch1==FALSE && !is.null(batches)){
        print("Batch correction for the 1st dataset was not selected.")
    }
    if (databatch2==TRUE){
        print("Run batch correction on the 2nd dataset")
        data2.batch %<-% BatchCorrect(data2,batch2,group2,technology[2])
        print("Batch correction of the second dataset finished")
    }else if (!is.null(data2) && databatch2==FALSE && !is.null(batches)){
        print("Batch correction for the 2nd dataset was not selected.")
    }
    if (is.null(batches)){
        print("Batch correction wasn't selected")
        }

    ###saving the results
    if(databatch1==TRUE && !is.null(data1.batch)){
        dir.create("BatchCorrect")
        setwd("BatchCorrect/")
        save(data1.batch,file="data1_BatchCorrect.rda")
        setwd("../")
    }
    if(databatch2==TRUE && !is.null(data2.batch)){
        dir.create("BatchCorrect")
        setwd("BatchCorrect/")
        save(data2.batch,file="data2_BatchCorrect.rda")
        setwd("../")
    }


    print("BATCH CORRECTION PART FINISHED")


    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    ### Distributional Checks ###
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

    print("PROCESSING DISTRIBUTIONAL CHECK")

    if (data.check == TRUE) {

        dir.create("DataChecks")
        setwd("DataChecks/")
        dir.create("data1/")
        setwd("data1/")

        if (databatch1 == TRUE) {
            mean.counts.data1<-meanCounts(data1.batch, group1)
            subset.data1 <- data1.batch[sample(nrow(data1.batch), 10),]
        } else {
            if (technology[1] == "seq") {
                mean.counts.data1<-meanCounts(data1$E, group1)
                subset.data1 <- data1$E[sample(nrow(data1$E), 10),]
            } else {
                mean.counts.data1<-meanCounts(data1, group1)
                subset.data1 <- data1[sample(nrow(data1), 10),]
            }
        }

        list.of.dists <- FitDistributions(subset.data1)
        PlotDistributions(subset.data1, list.of.dists)

        export_list(mean.counts.data1, file = "%s.csv")

        setwd("../")

    }

    if (data.check == TRUE & !is.null(data2)) {

        dir.create("data2/")
        setwd("data2/")

        if (databatch2 == TRUE) {
            mean.counts.data2<-meanCounts(data2.batch, group2)
            subset.data2 <- data2.batch[sample(nrow(data2.batch), 10),]
        } else {

            if (technology[2] == "seq") {
                mean.counts.data2<-meanCounts(data2$E, group2)
                subset.data2 <- data2$E[sample(nrow(data2$E), 10),]
            } else {
                mean.counts.data2<-meanCounts(data2, group2)
                subset.data2 <- data2[sample(nrow(data2), 10),]
            }
        }

        list.of.dists <- FitDistributions(subset.data2)
        PlotDistributions(subset.data2, list.of.dists)

        write.table(data.frame(mean.counts.data2), paste0(prefix,"_meanCounts_dataset2.txt"), sep = "\t", row.names=FALSE, col.names=TRUE, quote=FALSE)
        export_list(mean.counts.data2, file = "%s.csv")

        setwd("../")

    }

    setwd("../")

    print("DISTRIBUTIONAL CHECK PART FINISHED")


    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    ### PRELIMINARY PCA PLOT ###
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

    print("PROCESSING PRELIMINARY PCA PLOT")


    if (plot.PCA == TRUE){
        dir.create("PCA")
        setwd("PCA/")
        if (databatch1 == TRUE){
            PCAPlot(data1.batch, group1, show.PCA.labels, colors, paste0(prefix,"_1st_dataset"))
        } else {
            if (technology[1] == "seq") {
                PCAPlot(data.frame(data1$E), group1, show.PCA.labels, colors, paste0(prefix,"_1st_dataset"))
            } else {
                PCAPlot(data1$E, group1, show.PCA.labels, colors, paste0(prefix,"_1st_dataset")) ##not sure if this is necessary
            }
        }

        #processing second dataset
        if (!is.null(data2)){
            if (databatch2 == TRUE){
                PCAPlot(data2.batch, group2, show.PCA.labels, colors, paste0(prefix,"_2nd_dataset"))
            } else {
                if (technology[2] == "seq") {
                    PCAPlot(data.frame(data2$E), group2, show.PCA.labels, colors, paste0(prefix,"_2nd_dataset"))
                } else {
                    PCAPlot(data2$E, group2, show.PCA.labels, colors, paste0(prefix,"_2nd_dataset"))
                }
            }
        }
        setwd("..")
    } else {
        cat("\n- No preliminary PCA plot requested.\n")
    }

    print("PRELIMINARY PCA PLOT PART FINISHED")



    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    ### Kmeans ###
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    print("PROCESSING KMEANS")


    if (kmeans == TRUE){
        dir.create("kmeans")
        setwd("kmeans/")
        if (databatch1 == TRUE){
            Kmeans.Out1 <- runKmeans(data1.batch, show.PCA.labels, colors, paste0(prefix,"_1st_dataset"),num.km.clusters)
        } else {
            if (technology[1] == "seq") {
                Kmeans.Out1 <- runKmeans(data.frame(data1$E), show.PCA.labels, colors, paste0(prefix,"_1st_dataset"),num.km.clusters)
            } else {
                Kmeans.Out1 <- runKmeans(data1$E, show.PCA.labels, colors, paste0(prefix,"_1st_dataset"),num.km.clusters) ##not sure if this is necessary
            }
        }

        kmeans.clusters1 <- cbind(metadata1, Kmeans.Out1$res.clusters) ##here the result is a cluster information for each sample appended to the metadata file
        write.table(kmeans.clusters1, paste0(prefix,"_Metadata_Kmeans_dataset1.txt"), sep = "\t", row.names=FALSE, col.names=TRUE, quote=FALSE)
        pca.data1<-Kmeans.Out1$res.pca
        save(pca.data1,file="pca_data1.rda")

        ##kmeans for dataset2
        if(!is.null(data2)){
            if (databatch2 == TRUE){
                Kmeans.Out2 <- runKmeans(data2.batch, show.PCA.labels, colors, paste0(prefix,"_2nd_dataset"),num.km.clusters)
            } else {
                if (technology[1] == "seq") {
                    Kmeans.Out2 <- runKmeans(data.frame(data2$E), show.PCA.labels, colors, paste0(prefix,"_2nd_dataset"),num.km.clusters)
                } else {
                    Kmeans.Out2 <- runKmeans(data2$E, show.PCA.labels, colors, paste0(prefix,"_2nd_dataset"),num.km.clusters) ##not sure if this is necessary
                }
            }

            kmeans.clusters2 <- cbind(metadata2, Kmeans.Out2$res.clusters) ##here the product is cluster information for each sample
            write.table(kmeans.clusters2, paste0(prefix,"_Metadata_Kmeans_dataset2.txt"), sep = "\t", row.names=FALSE, col.names=TRUE, quote=FALSE)
            pca.data2<-Kmeans.Out2$res.pca
            save(pca.data2,file="pca_data2.rda")

        }

        setwd("..")

    } else {
        cat("\n- No k-means clustering requested.\n")
    }

    print("KMEANS PART FINISHED")


    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    ### DIFFERENTIAL EXPRESSION ANALYSIS ###
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

    print("PROCESSING DIFFERENTIAL EXPRESSION")

    # Differential Expression Analysis with Limma
    dir.create("Results_DEA")
    setwd("Results_DEA/")
    dir.create("data1")
    setwd("data1/")

    #First dataset
    DEARes1 <- RunDEA(data1, metadata1, group1, batch1, covarDEA1, cutoff.logFC1, cutoff.FDR1, paste0(prefix,"1"), block1)

    setwd("../")

    DEA1.out<-DEARes1$DEA.out  ##This is the main output from DEA; used in many other steps (heatmaps, DEA vs LASSO comparison, WGCNA, PPI, GmiRIs)
    DEA1.out<-(DEA1.out[order(DEA1.out$logFC, decreasing = TRUE), ])

    if (length(unique(DEA1.out$comparison)) > 1){
        DEA1.out<-subset(DEA1.out, grepl(control.group1, comparison, fixed = TRUE))  ##keep only comparisons of a specific group, usually a control group
    }

    res.DEA1.names<-DEARes1$res.DEA.names  ##feature names


    #Second dataset
    if(!is.null(data2) & !is.null(metadata2)) {

        dir.create("data2")
        setwd("data2/")

        DEARes2 <- RunDEA(data2, metadata2, group2, batch2, covarDEA2, cutoff.logFC2, cutoff.FDR2, paste0(prefix,"2"), block2)

        setwd("../")

        DEA2.out<-DEARes2$DEA.out  ##This is the main output from DEA; used in many other steps (heatmaps, DEA vs LASSO comparison, WGCNA, PPI, GmiRIs)
        DEA2.out<-(DEA2.out[order(DEA2.out$logFC, decreasing = TRUE), ])


        if (length(unique(DEA2.out$comparison)) > 1){
            DEA2.out<-subset(DEA2.out, grepl(control.group2, comparison, fixed = TRUE))
        }

        res.DEA2.names<-DEARes2$res.DEA.names  ##feature names
    }

    setwd("..")

    print("DIFFERENTIAL EXPRESSION PART DONE")



    #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    #                                                                                       ## Add gene names ###
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    print("ADD GENE NAME")

    DEA1.out.HUGO <- AddGeneName(DEA1.out,ensembl.version)

    if(!is.null(data2)){
        DEA2.out.HUGO <- AddGeneName(DEA2.out,ensembl.version)
    }

    print("ADD GENE NAME FINISHED")


    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    ### VISUALISATIONS FOR DIFFERENTIAL EXPRESSION ANALYSIS ###
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


    if (!isFALSE(plot.DEA)){
        print("PROCESSING VISUALISATIONS FOR DIFFERENTIAL EXPRESSION ANALYSIS")

        print("Processing 1st dataset")
        setwd("Results_DEA/data1/")
        RunDEAVisuals(DEA1.out.HUGO, cutoff.FDR1, cutoff.logFC1, n.labeled.features=15, control.group1, prefix)
        setwd("../../")
        if(!is.null(data2)){
            print("Processing 2nd dataset")
            setwd("Results_DEA/data2/")
            RunDEAVisuals(DEA2.out.HUGO, cutoff.FDR2, cutoff.logFC2, n.labeled.features=15, control.group2, prefix)
            setwd("../../")
        }
        print("VISUALIZATION FOR DIFFERENTIAL EXPRESSION FINISHED")
    }


    #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    #                                                                                       ## LASSO Regression ###
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


    print("PROCESSING LASSO")

    if (!isFALSE(alpha.lasso)){
        dir.create("LASSO_EN_Ridge")
        setwd("LASSO_EN_Ridge/")
        if (databatch1 == TRUE){
            LASSO1.results <- runLASSO(data1.batch, group1, alpha.lasso, min.coef.lasso, nfolds.lasso, prefix)
        } else {
            LASSO1.results <- runLASSO(data1$E, group1, alpha.lasso, min.coef.lasso, nfolds.lasso, prefix)
        }

        print("Analysing DEA vs LASSO consensus1")

        consensus1<-RunDEA_LASSO_consensus(DEA1.out, LASSO1.results, group1, viridis.palette="turbo", paste0(prefix,"_1"))  ##features are sorted based on DEA.out (sorted based on logFC)
        write.table(consensus1, paste0(prefix,"_DEA_LASSO_consensus1.txt"), sep = "\t", row.names=FALSE, col.names=TRUE, quote=FALSE)
        write.table(LASSO1.results$VarsSelect, paste0(prefix,"_LASSO1.txt"), sep = "\t", row.names=FALSE, col.names=TRUE, quote=FALSE)


        #processing second dataset
        if (!is.null(data2)){
            if (databatch2 == TRUE){
                LASSO2.results <- runLASSO(data2.batch, group2, alpha.lasso, min.coef.lasso, nfolds.lasso, prefix)
            } else {
                LASSO2.results <- runLASSO(data2, group2, alpha.lasso, min.coef.lasso, nfolds.lasso, prefix)
            }
            print("Analysing DEA vs LASSO consensus2")
            consensus2<-RunDEA_LASSO_consensus(DEA2.out, LASSO2.results, group2, viridis.palette="turbo", paste0(prefix,"_2"))
            write.table(consensus2, paste0(prefix,"_DEA_LASSO_consensus2.txt"), sep = "\t", row.names=FALSE, col.names=TRUE, quote=FALSE)
            write.table(LASSO2.results$VarsSelect, paste0(prefix,"_LASSO2.txt"), sep = "\t", row.names=FALSE, col.names=TRUE, quote=FALSE)

        }else{
            LASSO2.results<-NA
        }


        ##Saving AUC/ROC - only if all the sample groups are large enough
        print("saving roc")

        ###save AUC results
        if(!is.na(LASSO1.results$roc.res)){
            write.table(LASSO1.results$roc.res, paste0(prefix,"_AUC1.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)
        }else{
            print("ROC for dataset1 are not available")

        }
        if(!is.na(LASSO2.results)){
            if(!is.na(LASSO2.results$roc.res)){
                write.table(LASSO2.results$roc.res, paste0(prefix,"_AUC2.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)
            }else{
                print("ROC for dataset2 are not available")
            }
        }

        print("saving cv.error")

        ###save cross validation error rate results - only if all the sample groups are large enough
        if(!is.na(LASSO1.results$cv.error)){
            write.table(LASSO1.results$cv.error, paste0(prefix,"_crossVal_error_1.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)
        }else{
            print("Cross validation error rate for dataset1 are not available")
        }

        if(!is.na(LASSO2.results)){
            if(!is.na(LASSO2.results$cv.error)){
                write.table(LASSO2.results$cv.error, paste0(prefix,"_crossVal_error_2.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)
            }else{
                print("Cross validation error rate for dataset2 is not available")
            }
        }

        setwd("..")


    } else {
        cat("\n- LASSO/EN/Ridge regression not requested.\n")
    }

    print("LASSO/ELASTIC NETWORK/RIDGE REGRESSION ANALYSIS FINISHED.")




    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    ### PLOTTING HEATMAP ###
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


    if (!isFALSE(plot.heatmap)) {

        print("PRINTING HEATMAP")

        print("Processing 1st dataset")
        dir.create("Heatmaps")
        setwd("Heatmaps")
        dir.create("data1")
        setwd("data1")
        if (databatch1 == TRUE){
            heatmap.input1 <- data1.batch
        } else {
            if (technology[1] == "seq"){
                heatmap.input1 <- as.data.frame(data1$E)
            }else{
                heatmap.input1 <- as.data.frame(data1)
            }
        }

        if(plot.heatmap == "DEA"){
            RunHeatmap(heatmap.input1, DEA1.out, heatmap.size, group1, viridis.palette, plot.heatmap, prefix,data.type=paste0("expression/abundance")) ##DEA.out is sorted based on logFC
        } else if (plot.heatmap =="Consensus"){
            RunHeatmap(heatmap.input1, consensus1, heatmap.size, group1, viridis.palette, plot.heatmap, prefix,data.type=paste0("expression/abundance"))  ##Consensus is sorted based on DEA.out
        } else if (plot.heatmap %in% c("EN", "LASSO", "Ridge")){  ##Here it's difficult to sort the features based on their significance as they are generated based on an intersection of several LASSO runs
            RunHeatmap(heatmap.input1, LASSO1.results, heatmap.size, group1, viridis.palette, plot.heatmap, prefix,data.type=paste0("expression/abundance"))
        } else {
            stop(paste0("plot.heatmap value ", plot.heatmap, " is not supported. Supported values are: DEA, Consensus, EN, LASSO and Ridge."))
        }
        setwd("../../")


        if (!is.null(data2)){
        print("Processing 2nd dataset")
        dir.create("Heatmaps")
        setwd("Heatmaps")
        dir.create("data2")
        setwd("data2")
        if (databatch2 == TRUE){
            heatmap.input2 <- data2.batch
        } else {
            if (technology[2] == "seq"){
                heatmap.input2 <- as.data.frame(data2$E)
            }else{
                heatmap.input2 <- as.data.frame(data2)
            }
        }

        if(plot.heatmap == "DEA"){
            RunHeatmap(heatmap.input2, DEA2.out, heatmap.size, group2, viridis.palette, plot.heatmap, prefix,data.type=paste0("counts")) ##DEA.out is sorted based on logFC
        } else if (plot.heatmap =="Consensus"){
            RunHeatmap(heatmap.input2, consensus2, heatmap.size, group2, viridis.palette, plot.heatmap, prefix,data.type=paste0("counts"))  ##Consensus is sorted based on DEA.out
        } else if (plot.heatmap %in% c("EN", "LASSO", "Ridge")){  ##Here it's difficult to sort the features based on their significance as they are generated based on an intersection of several LASSO runs
            RunHeatmap(heatmap.input2, LASSO2.results, heatmap.size, group2, viridis.palette, plot.heatmap, prefix,data.type=paste0("counts"))
        } else {
            stop(paste0("plot.heatmap value ", plot.heatmap, " is not supported. Supported values are: DEA, Consensus, EN, LASSO and Ridge."))
        }
        setwd("../../")

        }




    print("CREATING HEATMAPS FINISHED")
    } else {
            cat("\n- No heatmap requested.\n")
    }



    #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    #                                                                            ### RANDOM FOREST ###
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



    print("PROCESSING RANDOM FOREST")

    if (!is.null(num.trees.init) & !is.null(num.trees.iterat)) {

      dir.create("random_forest")
      setwd("random_forest/")

      ## Run random forest

      ## Processing first dataset

      if (databatch1 == TRUE) {

         # Run random forest of data1.batch
         RF1.results <- RunRF(data = data1.batch, group = group1, split.size = split.size, test.train.ratio = test.train.ratio, num.trees.init = num.trees.init, num.trees.iterat = num.trees.iterat)

      } else {

         # Run random forest of data1$E
         RF1.results <- RunRF(data = data1$E, group = group1, split.size = split.size, test.train.ratio = test.train.ratio, num.trees.init = num.trees.init, num.trees.iterat = num.trees.iterat)

      }

      # Processing second dataset
      if (!is.null(data2)) {
        if (databatch2 == TRUE){

          # Run random forest on data2.batch
          RF2.results <- RunRF(data = data2.batch, group = group2, split.size = split.size, test.train.ratio = test.train.ratio, num.trees.init = num.trees.init, num.trees.iterat = num.trees.iterat)

        } else {

          # Run random forest on data2
          RF2.results <- RunRF(data = data2, group = group2, split.size = split.size, test.train.ratio = test.train.ratio, num.trees.init = num.trees.init, num.trees.iterat = num.trees.iterat)

        }

      } else {

        RF2.results <- NA

      }


      ## Save results for data1

      # Save intersected selected features
      if (!any(is.na(RF1.results$VarsSelect))) {

        # Save intersected selected features
        write.table(RF1.results$VarsSelect, paste0(prefix,"_intersected_selected_features_data1_rf.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)

      }

      # Save selected features from feature selection process from each seed run
      if (!any(is.na(RF1.results$RFResults$Sel.vars))) {

          # Bind selected features from each seed run into table
          sel.vars <- t(sapply(RF1.results$RFResults$Sel.vars, "length<-", max(lengths(RF1.results$RFResults$Sel.vars))))

          # Save selected features
          write.table(sel.vars, paste0(prefix,"_selected_features_data1_rf.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)

      }

      # Save OOB errors for all random forests from feature selection procedure for each seed run
      if (!any(is.na(RF1.results$RFResults$Var.sel.oob))) {

        # Bind OOB errors into table
        oob.table <- do.call(rbind, RF1.results$RFResults$Var.sel.oob)

        # Save table of OOB errors
        write.table(oob.table, paste0(prefix,"_oob_feature_selection_data1_rf.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)

      }

      # Save OOB error from the best random forest model (i.e. the final selected model)
      if (!any(is.na(RF1.results$RFResults$Sel.rf.oob))) {

          # Bind OOB errors into table
          sel.vars.oob <- do.call(rbind, RF1.results$RFResults$Sel.rf.oob)

          # Save OOB error from best random forest model (i.e. the final selected model)
          write.table(sel.vars.oob,
                      paste0(prefix,"_oob_final_selected_model_data1_rf.txt"), row.names=TRUE, col.names = TRUE, quote = FALSE)

      }

      # Save average feature initial importance during feature selection process (average across seed runs)
      if (!any(is.na(RF1.results$RFResults$Mean.importance.features))) {

        # Save average feature importance
        write.table(RF1.results$RFResults$Mean.importance.features,
                    paste0(prefix,"_mean_importance_feature_selection_data1_rf.txt"), row.names=TRUE, col.names = TRUE, quote = FALSE)

      }

      # Save OOB errors from fitted random forest model for each seed run
      if (!any(is.na(RF1.results$RFResults$oob.rf.model))) {

        # Bind OOB errors into table
        oob.table <- do.call(rbind, RF1.results$RFResults$oob.rf.model)

        # Save table of OOB errors
        write.table(oob.table, paste0(prefix,"_oob_model_data1_rf.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)

      }

      # Save accuracy and 95% CI for predictions of test data using fitted random forest model for each seed run
      if (!any(is.na(RF1.results$RFResults$accuracy.rf.model))) {

        # Save table of accuracies and 95% CI
        write.table(RF1.results$RFResults$accuracy.rf.model, paste0(prefix,"_accuracy_model_data1_rf.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)

      }

      # Save seeds
      if (!any(is.na(RF1.results$RFResults$seeds))) {

        # Save seeds
        write.table(RF1.results$RFResults$seeds, paste0(prefix,"_seeds_data1_rf.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)

      }


      ## Save results for data2

      if (!any(is.na(RF2.results))) {

        # Save intersected selected features
        if (!any(is.na(RF2.results$VarsSelect))) {

          # Save intersected selected features
          write.table(RF2.results$VarsSelect, paste0(prefix,"_intersected_selected_features_data2_rf.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)

        }

        # Save selected features from feature selection process from each seed run
        if (!any(is.na(RF2.results$RFResults$Sel.vars))) {

            # Bind selected features from each seed run into table
            sel.vars <- t(sapply(RF2.results$RFResults$Sel.vars, "length<-", max(lengths(RF2.results$RFResults$Sel.vars))))

            # Save selected features
            write.table(sel.vars, paste0(prefix,"_selected_features_data2_rf.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)

        }

        # Save OOB errors for all random forests from feature selection procedure for each seed run
        if (!any(is.na(RF2.results$RFResults$Var.sel.oob))) {

          # Bind OOB errors into table
          oob.table <- do.call(rbind, RF2.results$RFResults$Var.sel.oob)

          # Save table of OOB errors
          write.table(oob.table, paste0(prefix,"_oob_feature_selection_data2_rf.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)

        }

        # Save OOB error from the best random forest model (i.e. the final selected model)
        if (!any(is.na(RF2.results$RFResults$Sel.rf.oob))) {

            # Bind OOB errors into table
            sel.vars.oob <- do.call(rbind, RF2.results$RFResults$Sel.rf.oob)

            # Save OOB error from best random forest model (i.e. the final selected model)
            write.table(sel.vars.oob,
                          paste0(prefix,"_oob_final_selected_model_data2_rf.txt"), row.names=TRUE, col.names = TRUE, quote = FALSE)

        }

        # Save average feature initial importance during feature selection process (average across seed runs)
        if (!any(is.na(RF2.results$RFResults$Mean.importance.features))) {

          # Save average feature importance
          write.table(RF2.results$RFResults$Mean.importance.features,
                      paste0(prefix,"_mean_importance_feature_selection_data2_rf.txt"), row.names=TRUE, col.names = TRUE, quote = FALSE)

        }

        # Save OOB errors from fitted random forest model for each seed run
        if (!any(is.na(RF2.results$RFResults$oob.rf.model))) {

          # Bind OOB errors into table
          oob.table <- do.call(rbind, RF2.results$RFResults$oob.rf.model)

          # Save table of OOB errors
          write.table(oob.table, paste0(prefix,"_oob_model_data2_rf.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)

        }

        # Save accuracy and 95% CI for predictions of test data using fitted random forest model for each seed run
        if (!any(is.na(RF2.results$RFResults$accuracy.rf.model))) {

          # Save table of accuracies and 95% CI
          write.table(RF2.results$RFResults$accuracy.rf.model, paste0(prefix,"_accuracy_model_data2_rf.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)

        }

        # Save seeds
        if (!any(is.na(RF2.results$RFResults$seeds))) {

          # Save seeds
          write.table(RF2.results$RFResults$seeds, paste0(prefix,"_seeds_data2_rf.txt"), row.names=FALSE, col.names = TRUE, quote = FALSE)

        }

      }

      setwd("..")

    } else {

      cat("\n- Random forest not requested.\n")

    }

    print("RANDOM FOREST DONE")

    setwd("../")
    cat("\nCAMPP RUN DONE!\n")
}
